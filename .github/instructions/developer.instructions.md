---
applyTo: "backlog/tasks/**/*.md, src/**/*.{py,js,ts}, tests/**/*.{py,js,ts}, logs/**/*.md"
description: "Auto-validation rules for Developer Mode - enforces quality standards and test requirements"
autoLoad: true
---

# Developer - Auto-Validation Rules

> **Purpose:** These rules automatically validate quality during development. They complement the streamlined 5-phase workflow in `developer.chatmode.md`.

## ğŸ“ Applied To

```
âœ… backlog/tasks/**/*.md    â†’ Task file structure validation
âœ… src/**/*.{py,js,ts}      â†’ Code quality validation
âœ… tests/**/*.{py,js,ts}    â†’ Test completeness validation
âœ… logs/ERROR-TASK-*.md     â†’ Error log format validation
```

---

## ğŸ” Validation Rules

### Rule 1: Task File Must Be Complete (Phase 1)

**File Naming Pattern:**
```
TASK-{FEATURE-ID}-{TASK-NUM}-{slug}.md

âœ… Valid: TASK-001-001-create-user-model.md
âŒ Invalid: task-1-setup.md
```

**Required Sections:**
```markdown
MANDATORY:
âœ… ## Description
âœ… ## Technical Specification
   - Files to Create/Modify
   - Implementation Details (with code examples)
âœ… ## Test Plan (unit + integration specs)
âœ… ## Acceptance Criteria
âœ… ## Definition of Done
âœ… ## ASRs (if architecture-significant)

OPTIONAL:
â—‹ ## Dependencies
â—‹ ## Edge Cases
```

**Validation Error Format:**
```
âŒ Task File Validation Failed

File: backlog/tasks/FEATURE-001/task-setup.md
Issues: 2

1. âŒ Invalid filename
   Found: task-setup.md
   Expected: TASK-001-XXX-setup-xyz.md

2. âŒ Missing Test Plan section
   Cannot proceed without test specifications
   
Action: Notify @architect to complete task file
```

---

### Rule 2: Code Must Meet Quality Standards (Phase 2)

**Enforced Standards:**
```python
âœ… Type hints on all functions/methods
âœ… Docstrings (Google/NumPy style)
âœ… Input validation before processing
âœ… Error handling with logging (not print())
âœ… No hardcoded secrets or config
âœ… No TODOs or placeholders
âœ… No commented-out code

âŒ Violations trigger quality check failure
```

**Quality Check Examples:**

```python
# âŒ BAD - Missing type hints, bare except, print(), TODO
def process_user(data):
    try:
        user = data['user']
        print(user)  # Debug
        # TODO: Add validation
        return user
    except:
        return None

# âœ… GOOD - Type hints, docstring, validation, logging
import logging
from typing import Dict, Optional

logger = logging.getLogger(__name__)

def process_user(data: Dict) -> Optional[User]:
    """Process user data from request.
    
    Args:
        data: Request data with user info
    Returns:
        User object if valid
    Raises:
        ValueError: If user data invalid
    """
    if 'user' not in data:
        logger.error("Missing 'user' key")
        raise ValueError("User data required")
    
    try:
        user = User.from_dict(data['user'])
        logger.info(f"Processed user: {user.id}")
        return user
    except ValidationError as e:
        logger.error(f"Validation failed: {e}")
        raise
```

**Validation Error Format:**
```
âš ï¸ Code Quality Issues

File: src/users.py | Function: process_user
Issues: 4

1. âŒ No type hints (line 1)
   Fix: def process_user(data: Dict) -> Optional[User]:

2. âŒ Missing docstring
   Fix: Add Google/NumPy style docstring

3. âŒ Bare except clause (line 5)
   Fix: except ValidationError as e: + logging

4. âŒ print() instead of logging (line 4)
   Fix: logger.info(f"Processing user: {user.id}")

Action: Refactor per clean code principles
```

---

### Rule 3: Tests Must Be Complete (Phase 2-3)

**Required Coverage:**
```
âœ… ALL test cases from Task Test Plan
âœ… Minimum: 1 happy path + 1 edge case + 1 error case
âœ… Descriptive test names
âœ… Test docstrings
âœ… Proper assertions (no empty tests)
âœ… Parametrized tests for multiple scenarios

âŒ NO placeholder tests ("pass" or "TODO")
âŒ NO tests without assertions
âŒ NO skipping test implementation
```

**Test Quality Examples:**

```python
# âŒ BAD - Placeholder, no assertions
def test_user():
    pass  # TODO

def test_user2():
    user = create_user()  # No assertion!

# âœ… GOOD - Complete tests
import pytest

class TestUserCreation:
    """User creation test suite."""
    
    def test_valid_user_creation(self):
        """Test creating user with valid data."""
        user = create_user(
            email="test@example.com",
            password="SecurePass123!"
        )
        assert user.email == "test@example.com"
        assert user.is_active is True
    
    def test_invalid_email_raises_error(self):
        """Test error on invalid email."""
        with pytest.raises(ValidationError, match="Invalid email"):
            create_user(email="invalid", password="Pass123!")
    
    @pytest.mark.parametrize("email,password,error", [
        ("", "Pass!", "Email required"),
        ("test@ex.com", "", "Password required"),
    ])
    def test_validation_errors(self, email, password, error):
        """Test various validation scenarios."""
        with pytest.raises(ValidationError, match=error):
            create_user(email=email, password=password)
```

**Validation Error Format:**
```
âŒ Test Suite Incomplete

File: tests/unit/test_users.py | Function: create_user
Issues: 3

1. âŒ Missing tests from Test Plan
   Found: 2 tests | Required: 5 tests
   Missing:
   - test_invalid_email_raises_error()
   - test_weak_password_raises_error()
   - test_validation_errors() (parametrized)

2. âŒ No edge case tests
   Add: Empty input, boundary conditions

3. âŒ Placeholder test found (line 45)
   def test_user(): pass  # TODO
   
Action: Implement ALL tests from Task Test Plan (MANDATORY)
```

---

### Rule 4: Test Execution is MANDATORY (Phase 3)

**Required Execution:**
```bash
# Use project-appropriate test commands (adapt to your stack):
# See developer.chatmode.md for complete command reference

âœ… Quick check: Run affected tests
âœ… Full validation: Run ALL tests with coverage (MANDATORY)
âœ… Quality check: Run linter/formatter

# Supports: Python, JavaScript/TypeScript, Java, Go, C#, and more

âŒ NEVER skip test execution
âŒ NEVER assume tests pass
âŒ NEVER commit without running tests
```

**Documentation Required:**
```
Test Results:
âœ… Unit: X passed | Integration: Y passed | Coverage: Z%

OR if failed:
âŒ Unit: X/Y passed (Z FAILED)
â†’ Error log created: logs/ERROR-TASK-XXX-YYYY-MM-DD-HHMM.md
```

**Validation Error:**
```
âŒ CRITICAL: Tests Not Executed

Task: TASK-001-001-create-user-model
Status: Implementation complete âŒ Tests: NOT RUN

Action: Run canonical test suite (see chatmode)
Cannot proceed without test execution!
```

---

### Rule 5: Error Logs Required for Test Failures (Phase 3a)

**Error Log Requirements:**
```
File: logs/ERROR-TASK-XXX-YYYY-MM-DD-HHMM.md

MUST contain:
âœ… Error summary (count, type, severity)
âœ… Each test failure (file, error, stack trace, context)
âœ… Code context (implementation + test code)
âœ… Environment info (versions, dependencies)
âœ… Attempted solutions

MUST trigger:
âœ… STOP task execution
âœ… Notify @debugger
âœ… NO commit
âœ… NO proceed to next task
```

---

### 5. Test Failure Handling (MANDATORY!)

**Wenn Tests fehlschlagen:**

```markdown
CHECK bei Test-Failures:

âœ… Error Log erstellt?
âœ… Error Log Format korrekt?
âœ… Alle Failures dokumentiert?
âœ… Stack Traces enthalten?
âœ… Code Context enthalten?
âœ… Environment Info enthalten?
âœ… @debugger notified?
âœ… Task execution STOPPED?

MANDATORY Error Log Creation:
File: logs/ERROR-TASK-{FEATURE}-{TASK}-{YYYY-MM-DD}-{HHMM}.md

VERBOTEN:
âŒ Test-Failures ignorieren
âŒ "Will fix later"
âŒ Commit trotz failing tests
âŒ Continue to next task
âŒ Mark task as complete
```

**Error Log Template Required:**

```markdown
# Error Log: TASK-XXX-YYY

**Task ID:** TASK-XXX-YYY  
**Task Title:** [Title]  
**Date:** YYYY-MM-DD HH:MM  
**Developer:** Developer Mode  
**Status:** âŒ Tests Failed  

## Error Summary
**Failed Tests:** X out of Y tests failed  
**Test Type:** Unit | Integration | Both  
**Severity:** High | Medium | Low

## Test Failures
[Detaillierte Test-Failure-Info]

## Code Context
[Relevanter Code]

## Environment Information
[Environment Details]

## Attempted Solutions
[Was wurde versucht]

## Next Steps for @debugger
[Was Debugger analysieren soll]
```

**Fehlermeldung bei fehlender Error Log Creation:**

```
âŒ CRITICAL: Error Log nicht erstellt!

Test Results:
âŒ Unit Tests: 12/15 passed, 3 FAILED
âŒ Integration Tests: 5/8 passed, 3 FAILED

MANDATORY Action:
  Wenn Tests fehlschlagen, MUSS Error Log erstellt werden!

Action erforderlich:
  1. Erstelle: logs/ERROR-TASK-001-001-2025-10-07-1430.md
  2. Dokumentiere ALLE Test Failures
  3. Include Stack Traces
  4. Include Code Context
  5. Include Environment Info
  6. Notify @debugger
  7. STOP Task Execution

Template: .github/templates/ERROR-LOG-TEMPLATE.md

UNTIL Error Log created, task is BLOCKED!
```

---

### 6. Acceptance Criteria Validation

**Vor Marking Task Complete:**

```markdown
CHECK Acceptance Criteria:

âœ… Alle Criteria aus Task erfÃ¼llt?
âœ… Functionality wie spezifiziert?
âœ… Edge Cases behandelt?
âœ… Performance akzeptabel?
âœ… Keine Regressionen?

ALLE Criteria mÃ¼ssen âœ… sein:
- [ ] Criterion 1 aus Task
- [ ] Criterion 2 aus Task
- [ ] Criterion 3 aus Task
- [ ] ALL Tests passed
- [ ] Coverage >90%

VERBOTEN:
âŒ "Mostly works"
âŒ "Good enough"
âŒ UnerfÃ¼llte Criteria ignorieren
```

**Fehlermeldung bei unerfÃ¼llten Criteria:**

```
âš ï¸ Acceptance Criteria nicht erfÃ¼llt

Task: TASK-001-001-create-user-model
Status: Implementation complete
Acceptance Criteria: 3/5 âœ…

ErfÃ¼llte Criteria:
  âœ… User model has all required fields
  âœ… Email field has unique constraint
  âœ… Migration runs successfully

Nicht erfÃ¼llte Criteria:
  âŒ Timestamps auto-update
     â†’ created_at/updated_at nicht automatisch
  âŒ All tests pass
     â†’ 2 Tests failing

Action erforderlich:
  Fix unerfÃ¼llte Criteria:
  
  1. Timestamps auto-update:
     Add server_default=func.now() and onupdate=func.now()
  
  2. Fix failing tests:
     Run tests und behebe Failures

Task kann NICHT als complete markiert werden bis
ALLE Acceptance Criteria erfÃ¼llt sind!
```

---

### 7. Definition of Done Validation

**Final Check vor Commit:**

```markdown
CHECK Definition of Done:

âœ… Code implemented as specified?
âœ… Unit tests written and passing?
âœ… Integration tests written and passing?
âœ… Code reviewed (self-review)?
âœ… Documentation updated?
âœ… No TODOs oder Platzhalter?
âœ… Clean Code Principles befolgt?
âœ… BACKLOG.md Code-Mapping bereit?

ALLE Items mÃ¼ssen âœ… sein!

VERBOTEN:
âŒ "Almost done"
âŒ Skip DoD Items
âŒ "Will do later"
```

**Validation Error:**
```
âŒ Definition of Done Incomplete

Task: TASK-001-001 | DoD: 5/7 âœ…

Complete:
âœ… Code implemented
âœ… Tests passing
âœ… Self-reviewed
âœ… No TODOs

Incomplete:
âŒ Documentation not updated
âŒ BACKLOG.md not updated

Action: Complete DoD items before commit
```

---

### Rule 6: Commit Message Must Be Informative (Phase 4)

**Required Format:**
```
type(scope): TASK-XXX - Brief description

Implementation:
- Change 1
- Change 2

Testing:
- Unit: X passed | Integration: Y passed | Coverage: Z%

Closes TASK-XXX | Refs FEATURE-XXX
```

**Examples:**
```
âŒ BAD: "added user model"
   (no type, scope, task ref, testing info)

âœ… GOOD: feat(models): TASK-001-001 - User database model

Implementation:
- User model with email, password fields
- Unique email constraint
- Auto timestamps

Testing:
- Unit: 15 passed | Integration: 8 passed | Coverage: 94%

Closes TASK-001-001 | Refs FEATURE-001
```

---

## ğŸ¯ Non-Bypassable Rules

### Rule Alpha: NO Code Without Tests
```
âŒ NEVER write code without tests
âŒ NEVER commit untested code
âŒ NEVER "add tests later"

âœ… ALWAYS write tests during implementation
âœ… ALWAYS run tests before commit
```

### Rule Beta: NO Commit Without Passing Tests
```
âŒ NEVER commit with failing tests
âŒ NEVER skip test execution
âŒ NEVER mock test results

âœ… ALL tests MUST pass OR error log created
âœ… Coverage â‰¥90% maintained
âœ… @debugger notified if failures
```

### Rule Gamma: NO Task Complete Without Full Test Suite
```
âŒ NEVER mark task complete without tests
âŒ NEVER skip Test Plan implementation
âŒ NEVER incomplete test coverage

âœ… ALL Test Plan tests implemented
âœ… ALL tests executed
âœ… ALL tests passing OR error log + @debugger
```

---

### 8. Commit Message Validation

**Commit Message Quality Check:**

```markdown
CHECK Commit Message:

âœ… Type correct? (feat|fix|test|docs|refactor|chore)
âœ… Scope present?
âœ… Brief description clear?
âœ… Implementation section vorhanden?
âœ… Testing section vorhanden?
âœ… Test results dokumentiert?
âœ… References Task?
âœ… References Feature/Issue?

Format:
type(scope): TASK-XXX-YYY - Brief description

Implementation:
- Change 1
- Change 2

Testing:
- Unit tests: X passing
- Integration tests: Y passing
- Coverage: Z%

Closes TASK-XXX-YYY
References FEATURE-XXX, ISSUE-XXX
```

**Beispiel - BAD Commit:**

```
âŒ BAD:
"added user model"

Problems:
1. No type
2. No scope
3. No task reference
4. No testing info
```

**Beispiel - GOOD Commit:**

```
âœ… GOOD:
feat(models): TASK-001-001 - create User database model

Implementation:
- Created User model with email, password_hash fields
- Added unique constraint on email
- Added automatic timestamps (created_at, updated_at)
- Created Alembic migration for users table

Testing:
- Unit tests: 15/15 passing
- Integration tests: 8/8 passing
- Coverage: 94%

Closes TASK-001-001
References FEATURE-001, ISSUE-001
```

**Fehlermeldung bei schlechter Commit Message:**

```
âš ï¸ Commit Message unzureichend

Gefunden: "added user model"

Probleme:
1. âŒ Kein Type (feat|fix|...)
2. âŒ Kein Scope (models)
3. âŒ Kein Task Reference
4. âŒ Keine Implementation Details
5. âŒ Keine Testing Information
6. âŒ Keine References

Action erforderlich:
  Schreibe informative Commit Message mit:
  - Type und Scope
  - Task Reference
  - Implementation Details
  - Testing Results
  - References zu Feature/Issue

Template: .github/templates/COMMIT-MESSAGE-TEMPLATE.txt
```

---

## ğŸ¯ Test-Enforcement Rules (CANNOT BE BYPASSED)

**Diese Regeln sind NICHT optional:**

### Rule 1: No Code Without Tests

```
âŒ NICHT ERLAUBT:
- Code schreiben ohne Tests
- Tests "spÃ¤ter" schreiben
- Tests Ã¼berspringen

âœ… MANDATORY:
- Tests schreiben (Phase 4)
- Tests ausfÃ¼hren (Phase 5)
- Tests passing (Phase 5)
```

### Rule 2: ALL Tests Must Pass

```
âŒ NICHT ERLAUBT:
- Commit mit failing tests
- "Most tests pass"
- Failing tests ignorieren

âœ… MANDATORY:
- 100% Tests passing
- Error Log if failures
- @debugger notification
```

### Rule 3: Comprehensive Testing Required

```
âŒ NICHT ERLAUBT:
- Nur affected tests run
- Skip integration tests
- "Assume tests pass"

âœ… MANDATORY:
- ALL unit tests run
- ALL integration tests run
- Coverage check performed
```

### Rule 4: Error Logs for Failures

```
âŒ NICHT ERLAUBT:
- Continue without Error Log
- Skip Error Log creation
- "Will fix myself"

âœ… MANDATORY:
- Create Error Log immediately
- Document all failures
- Notify @debugger
- STOP task execution
```

### Rule 5: Coverage Maintained

```
âŒ NICHT ERLAUBT:
- Coverage drop below 90%
- "Coverage doesn't matter"
- Skip coverage check

âœ… MANDATORY:
- Maintain >90% coverage
- New code 100% coverage
- Coverage check in tests
```

---

---

## ğŸ¯ Quality Gates (Streamlined)

**Pre-Commit Validation:**

```
Phase 1: Task Analysis & Setup
  âœ… Task file valid + Test Plan present
  âœ… Dependencies ready + ASRs understood

Phase 2: Implementation
  âœ… Code follows spec + Clean code applied
  âœ… Tests written + No TODOs

Phase 3: Testing & Validation
  âœ… ALL tests executed + ALL passing
  âœ… Coverage â‰¥90% + No regressions

Phase 4: Validation & Commit
  âœ… Acceptance criteria met + DoD complete
  âœ… Commit message informative

Phase 5: Completion & Metrics
  âœ… BACKLOG.md updated + Metrics tracked

ALL âœ… â†’ Commit allowed
ANY âŒ â†’ BLOCK + Show specific failure
```

---

## ğŸš¨ Critical Blocks (Cannot Proceed)

```
âŒ Tests not written
   â†’ Write ALL tests from Test Plan (MANDATORY)

âŒ Tests not executed
   â†’ Run canonical test suite (MANDATORY)

âŒ Tests failing
   â†’ Create Error Log â†’ Notify @debugger â†’ STOP

âŒ Coverage <90%
   â†’ Add tests to increase coverage

âŒ Error log missing (when tests fail)
   â†’ Create complete error log (MANDATORY)
```

---

## ğŸ’¬ Validation Messages

**Success:**
```
âœ… {Phase Name}
Status: Ready for next phase
```

**Block:**
```
âŒ CRITICAL: {Phase} BLOCKED
Issue: {Description}
Action: {Required steps}
Cannot proceed until resolved
```

**Warning:**
```
âš ï¸ {Phase Name}
Issue: {Description}
Recommendation: {Suggestion}
Can proceed but not recommended
```

---

## ğŸ“Š Metrics & Continuous Improvement

**Track:**
- Task duration (start to commit)
- Test pass rate (first attempt %)
- Error log frequency
- Coverage trends
- Code quality scores

**Learn:**
- Analyze error patterns
- Identify common failures
- Suggest architectural improvements
- Optimize process

---

## ğŸ“Œ Summary

**Purpose:** Auto-validate quality during development

**Key Principles:**
1. âœ… Quality over speed
2. âœ… Testing is MANDATORY
3. âœ… Clean code enforced
4. âœ… Systematic error handling
5. âœ… Continuous improvement

**Core Rule:** **"No Code Ships Without Tests"**

---

**Version:** 4.0 (Streamlined Validation)  
**Last Updated:** 2025-11-02  
**Integration:** Works with developer.chatmode.md (5-Phase Workflow)

---

## ğŸš¨ Critical Validation Failures

**INSTANT BLOCKS (Cannot Proceed):**

1. **âŒ Tests Not Written**
   ```
   BLOCK: Cannot proceed past Phase 4 without tests
   REASON: Test-Driven Development is MANDATORY
   ACTION: Write ALL tests from Test Plan
   ```

2. **âŒ Tests Not Executed**
   ```
   BLOCK: Cannot mark task complete without test execution
   REASON: Test execution is MANDATORY
   ACTION: Run ALL tests (unit + integration + coverage)
   ```

3. **âŒ Tests Failing**
   ```
   BLOCK: Cannot commit with failing tests
   REASON: Only passing tests can be committed
   ACTION: Create Error Log â†’ Notify @debugger â†’ STOP
   ```

4. **âŒ Coverage Below 90%**
   ```
   BLOCK: Cannot proceed with insufficient coverage
   REASON: >90% coverage is MANDATORY
   ACTION: Add tests to increase coverage
   ```

5. **âŒ Error Log Not Created (when tests fail)**
   ```
   BLOCK: Cannot continue without Error Log
   REASON: Debugging requires documentation
   ACTION: Create complete Error Log â†’ Notify @debugger
   ```

---

## ğŸ’¬ Validation Message Formats

### Success Format:

```
âœ… {PHASE}

Validation successful:
  âœ… {Check 1}
  âœ… {Check 2}

Status: Ready for {Next Phase}
```

### Critical Block Format:

```
âŒ CRITICAL: {PHASE} BLOCKED

Blocking Issue:
  âŒ {Issue Description}

Impact: {Impact Description}

Action Required:
  1. {Specific Action 1}
  2. {Specific Action 2}

CANNOT PROCEED until resolved!
```

---

## ğŸ”„ Integration Points

### Mit Architect:

```
Architect erstellt Task
  â†“
Includes Test Plan (MANDATORY)
  â†“
Developer reads Task
  â†“
Validates Test Plan presence
  â†“
If missing â†’ Notify @architect
```

### Mit Debugger:

```
Developer runs tests
  â†“
Tests fail âŒ
  â†“
Create Error Log (MANDATORY)
  â†“
Notify @debugger
  â†“
STOP task execution
  â†“
Wait for @debugger fix
```

---

## ğŸ“‹ Zusammenfassung

Diese Instructions stellen sicher:

âœ… **Test-First Approach** - Tests sind MANDATORY, nicht optional  
âœ… **Quality Enforcement** - Clean Code + Type Hints + Validation  
âœ… **Comprehensive Testing** - ALL Tests run, >90% Coverage  
âœ… **Error Handling** - Error Logs bei Test-Failures  
âœ… **Documentation** - Code, Tests, Commits vollstÃ¤ndig dokumentiert  
âœ… **No Shortcuts** - Kein Code shipped ohne Tests  

**Ziel:** Stelle sicher, dass JEDER Task dem Quality-Standard entspricht - mit MANDATORY Testing als Fundament!

---

**Version:** 1.0  
**Last Updated:** 2025-10-07  
**Critical Feature:** Test-Enforcement (MANDATORY)  
**Integration:** Works with developer.chatmode.md and debugger.chatmode.md